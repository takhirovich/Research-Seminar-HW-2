Introduction to Neural Networks — Part 1  Neural Networks have become a huge hit in the recent Machine Learning craze due to their significantly better performance than traditional Machine Learning algorithms in many cases. The art and science of Deep Learning is built on the foundation of Neural Networks and how they work. Hence demystifying Neural Networks is going to be the first step in demystifying Deep Learning. Let’s dive in! What is a Neural Network?  How do we define a Neural Network? It is essentially a naive implementation of how our brains might work. It’s not a very accurate representation but it tries to replicate some of the methods our brain uses to learn from it’s mistakes. Let’s look at how our brains work from a simplified perspective and then compare it with a Neural Network.  The brain is essentially a bunch of neurons connected to each other in a huge interconnected network. There are a lot of neurons and even more connections. These neurons pass a small amount of electrical charge to each other as a way to transmit information. Another important feature of these neural connections is that the connection between two neurons can be vary between strong and weak. A strong connection allows more charge to flow between them and a weak one allows lesser. A neuron pathway which frequently transmits charge will eventually become a strong pathway.  Now as the brain takes input from any external source, let’s say for example we touch a hot pan. The nerves from our hand transmits info to certain neurons in our brain. Now there is a pathway from these neurons to the neurons which control our hand. And in these cases our brain has learnt that the best option is to move our hand from the pan ASAP. Hence this certain pathway between the neurons taking input from the hand and the neurons controlling the hand will be strong.  Neural pathways become stronger upon frequent usage, and our brain essentially tries to use pathways which have proven to give us better results over time. So essentially as we humans live our lives and decide whether our actions are good or bad, we are training our brain to make sure we don’t repeat our previous mistakes or keep doing things which we think resulted in a good outcome. This is a highly simplified explanation and doesn’t fully portray what’s going on, but hopefully it helps you understand the basic concept.  Functionality of a Neural Network  Now let’s understand how a Neural Network is represented. A neural network consists of many Nodes (Neurons) in many layers. Each layer can have any number of nodes and a neural network can have any number of layers. Let’s have a closer look at a couple of layers.    Layers in a Neural Network  Now as you can see, there are many interconnections between both the layers. These interconnections exist between each node in the first layer with each and every node in the second layer. These are also called the weights between two layers.  Now let’s see how exactly these weights function.    Here we take the example of what’s going on with a single node in the network. Here we are considering all the values from the previous layer connecting to one node in the next layer.  Y is the final value of the node.  W represents the weights between the nodes in the previous layer and the output node.  X represents the values of the nodes of the previous layer.  B represents bias, which is an additional value present for each neuron. Bias is essentially a weight without an input term. It’s useful for having an extra bit of adjustability which is not dependant on previous layer.  H is the intermediate node value. This is not the final value of the node.  f( ) is called an Activation Function and it is something we can choose. We will go through it’s importance later.  So finally, the output value of this node will be f(0.57)  Now let’s look at the calculations between two complete layers:    The weights in this case have been colour coded for easier understanding. We can represent the entire calculation as a matrix multiplication. If we represent the weights corresponding to each input node as vectors and arrange them horizontally, we can form a matrix, this is called the weight matrix. Now we can multiply the weight matrix with the input vector and then add the bias vector to get the intermediate node values.    We can summarize the entire calculation as Y = f(W*X + B). Here, Y is the output vector, X is the input vector, W represents the weight matrix between the two layers and B is the bias vector.  We can determine the size of the weight matrix by looking at the number of input nodes and output nodes. An M*N weight matrix means that it is between two layers with the first layer having N nodes and the second layer having M nodes.    Now let’s look at a complete neural network.    This is a small neural network of four layers. The input layer is where we feed our external stimulus, or basically the data from which our neural network has to learn from. The output layer is where we are supposed to get the target value, this represents what exactly our neural network is trying to predict or learn. All layers in between are called hidden layers. When we feed the inputs into the first layer, the values of the nodes will be calculated layer by layer using the matrix multiplications and activation functions till we get the final values at the output layer. That is how we get an output from a neural network.  So essentially a neural network is, simply put, a series of matrix multiplications and activation functions. When we input a vector containing the input data, the data is multiplied with the sequence of weight matrices and subjected to activation functions till it reaches the output layer, which contains the predictions of the neural network corresponding to that particular input.  Role of Activation Function  Even though our neural network has a very complex configuration of weights, it will not be able to solve a problem without the activation function. The reason for this lies in the concept of Non Linearity.  Let’s revise what linearity and non linearity means.    linear relationship example  The above equation represents a linear relationship between Y and X1,X2. Regardless of what values W1 and W2 have, at the end of the day the change of value of X1 and X2 will result in a linear change in Y. Now if we look at real world data we realize this is actually not desirable because data often has non linear relationships between the input and output variables.    linear relationship vs non-linear relationship  The above diagram represents a typical dataset which shows a non-linear relationship between X and Y. If we try to fit a linear relationship on the data, we will end up with the red line, which is not a very accurate representation of the data. However if our relationship can be non linear, we are able to get the green line, which is much better.  Now let’s compare the neural network equation with and without the activation function.    We can observe that in this equation, there exists a linear relationship between the input and the output. However in the case of the equation with activation function, we can say that the relationship between input and output can be non linear, IF the activation function is itself non linear. Hence all we have to do is keep some non linear function as the activation function for each neuron and our neural network is now capable of fitting on non linear data.  Let’s look at a couple of popular activation functions:    ReLU: ReLU stands for Rectified Linear Unit. It essentially becomes an identity function (y = x) when x ≥ 0 and becomes 0 when x < 0. This is a very widely used activation function because its a nonlinear function and it is very simple.  Sigmoid: Sigmoid is essentially a function bounded between 0 and 1. It will become 0 for values which are very negative and 1 for values which are very positive. Hence this function squishes values which are very high or very low to values between 0 and 1. This is useful in neural networks sometimes to ensure values aren’t extremely high or low. This function is usually used at the last layer when we need values which are binary (0 or 1).  This concludes this part of the tutorial. The next part will explain in detail how exactly we can use our data to train our neural network. Thank you for reading!    Introduction to Neural Networks — Part 2  Backpropagation  Now that we have seen how a neural network is represented, we can go on to see how exactly it works. Since there are many layers having many neurons, there exists a complex set of weights to get an output from some input variables. Each weight in this network can be changed and hence there are countless configurations a neural network can have. A trained neural network has some weights configuration which accurately predicts correct outputs from some input data and that is what we hope to achieve. We will now go through how exactly a neural network trains itself to get this desirable weight configuration.  Backpropagation is the name of the algorithm a neural network uses to train itself. This revolutionary algorithm is a mixture of the chain rule in derivation and gradient descent, which is a common optimization algorithm which is used in linear and logistic regression.  To understand how backpropagation works, first we have to understand the relationship between the output and the weights in between. It is clear that every weight in the neural network will affect the output in some way due to the way the neural network is connected. Due to this fact, we can say that if I change a particular weight, the output will change in some way. We can also find the exact mathematical equation defining the relationship between each weight and the output.    Let’s say we are given a dataset with three input features, (X1, X2 and X3) and we need to find the relationship between the input features and the output using a neural network. We will now see what exactly the neural network does.    First step is to feed in the data and getting the output from the neural network. We shall call the output, Y_pred. Next we should compare the predicted value with the actual value. This value will be called the error. It is essentially how bad or how far off the model is from predicting the values correctly. Our goal is to now minimize the error.  Since Y_pred is a function of all the weights in the model and Error is a function of Y_pred, we can say that the Error will also depend on the weights. This means that we need to adjust our weights in such a way that the error is minimized. We do that using partial derivatives.  Let’s take a simple example of an equation, Y = W1*X1 + W2*X2. If we find the partial derivative of W1 or W2 with respect to Y, we can find out how W1 or W2 can affect Y.  If the partial derivative of W1 with respect to Y is POSITIVE, that means DECREASING the weight will DECREASE Y.  If the partial derivative of W1 with respect to Y is NEGATIVE, that means INCREASING the weight will DECREASE Y.    These two rules are all we need to optimize the weights in the neural network. We need to find the partial derivative of every weight with respect to the error. If that partial derivative is positive, then we decrease the value of that weight so that error gets decreased. If that partial derivative is negative, then we increase that weight so that the error gets decreased. This is the basic underlying concept of how weights are updated after we calculate the error.  Since the last layer is the closest to the error, we will first derive the last layer with respect to the error and update those weights. Then we will move to the second last layer to do the same and so on and so forth. We repeat this process till we reach the first layer and all the weights are updated. This entire process is called backpropagation.    We perform backpropagation for a single row of data and update the weights. We then repeat for all the data available in the training data set, this entire cycle is called one epoch. Usually neural networks can take several epochs to train and it is up to us to decide how many epochs it will train for.  Error Calculation Methods  Error is a very important part of a neural network because it allows us to estimate how poorly the model is performing and accordingly we can update our weights to improve performance. Now let’s go through how exactly error can be calculated in neural networks. The type of error calculation method we choose will depend upon the type of task we are trying to do. There are mainly two types of tasks a typical neural network can do:  Regression  Classification  Regression is when our output variable is continuous in nature. When Y is a numerical variable which we have to predict, the task is called regression. Examples include trying to predict house prices or trying to predict how many marks someone will score in an exam.  Classification is when our output variable is discrete in nature. When Y is trying to represent a certain class out of a defined number of classes, the task is called classification. Examples include trying to predict between a cat and a dog, or trying to predict whether an email is spam or not.    Here, we have to understand that in the regression graph the output variable is a continuous value just like X. In the classification graph, there are 2 input variables (X1 and X2) and the output is represented by the color of the points. Eg: Y = 0 for RED and Y = 1 for GREEN.  Let’s first cover regression.  For regression, the error function we use is Mean Squared Error. We are simply trying to find the difference between the actual value and predicted value, and square it. The formula for the same is given below:    Here, E is the error of the model. Y pred is the output of the model for the data and Y is the actual output which we are supposed to get. We square this so that negative and positive differences both become positive in the end. N represents the number of data rows we input into the model. We are essentially calculating error of all points and averaging them. As we train the model we hope to achieve a very low MSE so that we know that our neural network is predicting values which are close to the actual values.  Moving on to classification.  In classification the way we represent data is completely different. Because we are dealing with multiple classes which may not be in a numerical format, we have to convert them into a numerical format so that the neural network can process them. We typically convert them using the method called one hot encoding.  Let’s take the example of having three cities as classes and we have to predict between them. San Francisco, New York and Boston. We will then have an array of size 3 representing Y for each row of data. It is done in the following way:    The array size will indicate the total number of classes and the position of 1 in the array will represent the class.  Before we calculate the error in this scenario. We also have to ensure that the last layer of the neural network has a number of neurons equal to the number of classes. The last layer of the neural network should also be having the activation function of softmax. This activation function essentially converts the last layer into a probability distribution. Hence the sum of all the values of the nodes in the last layer will be equal to 1.  (Note: If there are only two classes we can use just one node and use Sigmoid instead of Softmax)    Now that it’s a probability distribution and all the values are between 1 and 0, we can proceed to calculate the error. The error we use here is called Log Loss, or Categorical Cross Entropy. What this function basically does is to compare each element of Y pred and Y and see how far apart they are. Then we average this value across all elements.  From the classification perspective, when we predict 0 at a particular element but the actual value is 1, or when we predict 1 and the actual value is 0, that means our model is performing extremely bad and we have to have a huge error in this case. This is the main concept of this error function.    Having this kind of a error function forces the neural network to understand where it is going wrong much quicker and hence allows it to learn from the data much faster. I would advise you to go through the mathematical formulas of Softmax and Log Loss for better understanding of what’s going on.  This concludes this two part tutorial on Neural Networks. I will be going much deeper into some of the concepts we have covered today in future articles and I will be going through several technical tutorials as well. Thank you for reading!  Recently, the words Artificial Intelligence, Machine Learning, Deep Learning, Neural Networks are spreading like wildfire. So, what are these terms?Wikipedia defines “Artificial Intelligence” as intelligence demonstrated by machines. In other words, it is the study of ability of devices to interpret external data, learn from that data and use that data to achieve its goals.  “Machine Learning” is a subset of Artificial Intelligence, in which machines learn to complete a task, without being explicitly programmed to do so. The difference between traditional programming and machine learning is that in traditional programming, you pass an input and a set of rules to the machine, and the machine gives you an output, while in Machine Learning you pass an input and the output, and the machine learns the set of rules which formed the output, so that you can use those rules to get an output in future automatically.  “Deep Learning” is the subset of Machine Learning that is concerned with algorithms inspired by the structure and function of the brain called Neural Networks. We can think about it in terms of writing: If you had ten people write the same word, the word would look very different for each person, right from cursive to print, or sloppy to neat. The human brain has no problem understanding that it’s all the same, but how would a normal computer system know that? In other words, how could we make a machine act intelligently like the human brain? That brings us to Neural Networks. Before we try to learn what a Neural Network is, let us look into its most basic building block, i.e a Neuron.  What is a Neuron?  Like in a human brain, the basic building block of a Neural Network is a Neuron. Its functionality is similar to a human brain, i.e, it takes in some inputs and fires an output. Each neuron is a small computing unit that takes a set of real valued numbers as input, performs some computation on them, and produces a single output value.To understand the working of neuron, let us first understand the meaning of a few terms.  Weight : Every input(x) to a neuron has an associated weight(w), which is assigned on the basis of its relative importance to other inputs.  The way a neuron works is, if the weighted sum of inputs is greater than a specific threshold, it would give an output 1, otherwise an output 0. This is the mathematical model of a neuron, also known as the Perceptron.  Every neural unit takes in a weighted sum of its inputs, with an additional term in the sum called a Bias.  Bias: Bias is a constant which is used to adjust the output along with the weighted sum of inputs, so that the model can best fit for the given data.  It is easier to represent the weighted sum of inputs using vector notations, so we defined weighted sum z in terms of weight vector w, input vector x , and a bias value b.The output(y) of the neuron is a function f of the weighted sum of inputs z. The function f is non linear and is called the Activation Function.Activation Function: The purpose of activation function is to introduce non-linearity into the output of neuron. It takes a single number, and performs some mathematical operation on it. There are several activation functions used in practice:Every activation function introduces some property that makes it more advantageous than just a linear weighted sum of inputs. For example, ReLU is very close to linear and for very high values of z, the values of y are more varied than a sigmoid/tanh activation. Sigmoid/Tanh activation maps any outliers towards the mean value. In general practice, ReLU has been found to be performing better than sigmoid or tanh activations.  Neural Networks  A neural network is composed of layers, which is a collection of neurons, with connections between different layers. These layers transform data by first calculating the weighted sum of inputs and then normalizing it using the activation functions assigned to the neurons.The leftmost layer in a Neural Network is called the input layer, and the rightmost layer is called the output layer. The layers between the input and the output, are called the hidden layers. Any Neural Network has 1 input layer and 1 output layer. However, the number of hidden layers differ between different networks depending on the complexity of the problem. Also, each hidden layer can have its own activation function.  Any neural network with two or more than 2 hidden layers is called a Deep Neural Network. A neural network makes accurate predictions by learning the weights for each of the neurons at every layer. The algorithm through which they learn is called as “Back Propagation”, which I will cover in a later post.  With this, I hope you will have a basic understanding about a Neural Network, its basic structure and various jargons associated with it. The advancement in this area is increasing at a rapid pace, and it would be good to start learning about it now, because the hype is real!  The previous few years have made it very clear that AI as a next massive evolution that is an integral part of all the sectors. Since banking is one among the most data-intensive industries, its transition into AI infrastructure is crucial to surviving the next massive data revolution.  BANKING IN INDIA Banking in India has usually been an urban service industry, unlike other countries the penetration of banking in India has been very minimal. With the arrival of simple-to-use monetary transaction mobile applications such as Paytm, Google Pay et al. , there’s a large demand to make and maintain a bank account to run any form of business or to access any form of service regardless of whether one is in an urban or rural setting.  DEFINING MOMENT: RISE OF CASHLESS TRANSACTIONS With twin rise of smartphones and efficient payment gateway apps, there are an exponential rise cashless transactions across all the fields. This form of transactions leaves behind a huge quantity of unstructured data. These transactions leave behind a digital path that’s unprotected for any kind of data breach. Given the massive amount of change in client behavior and their monetary activity, most banks in India are not developed enough to handle this quantity of data traffic and keep up with data overload..  Here are few predictions for the approaching year to assist pave the path for better banking experience and efficiency.  BETTER CUSTOMER INTERACTION AND SATISFACTION One of the most widespread utilities of AI is to improve client engagement and retention is through automation. By utilizing AI to modify several of the administrative tasks staff do, those employees can then focus more of their time and energy on building relationships with customers and serving to customers with inventive solutions to issues. Automating user FAQ and personalized information feed will not help consumer experience but will also save time for the bank customer executives.  FASTER ACCOUNT KYC VERIFICATION KYC verification becomes an automated affair with artificial intelligence functioning at its core. Any attempt at meddling with personal data or employing a fake document to perform KYC will be detected in real-time with the help of a well-devised AI. this will modify the account verification method by cross-checking the given KYC document to the adjacent government records to cross verify the credibleness of the submitted documents.  PERSONALISED LOAN SANCTION Banks and financial establishments, with some notable exceptions, are combating bad loans. artificial intelligence can process massive amounts of data that human underwriters would simply not be able to make sense of. artificial intelligence brings the flexibility to capture and exploit patterns that are distinctive to the loan portfolios of various lenders  Many loan applicants have different consumer acquisition channels, loan underwriting models and collections processes. These variations lead to loan reimbursement patterns that are individual to them — patterns that ancient underwriting models don’t account for. making it crucial new solution to the ever existing bad loan problem that has been plaguing the banking system.  TRANSACTIONS GET MORE FOOL-PROOF  On an average economic value of cyber attacks stand at $10.4 million for banking organizations across India as well as direct value, indirect causes like jobs losses and economic science factors, says a report published in 2018. The solution lies in integrating cybersecurity with the product and looking at it as an enabler for digital transformation method. Banking institutes should additionally leverage AI and automation in cybersecurity to attain faster and accurate detection of threats. they’re much more capable than humans when it involves identifying the blind spots, protecting privileged accounts, detecting the cyber threat, responding to the ongoing threat and providing the recovery resolution. AI will create testing and vulnerability-scanning stronger using algorithms to close the gap between thinking something that is in production is unsafe and knowing it’s unsafe. this can be done by looking at each industry category (such as banking or retail) and examining the firewalls, endpoint and alternative security products you are using and how they’re designed in your overall security stack.  FRAUD DETECTION WILL BE MORE INTELLIGENT  Earlier, customers had restricted channels available to interact and interact with their banks, which is why even the frauds in the banking system were restricted to loan defaults and theft. The proliferation of technology, however, unfolded varied channels of interactions and transactions — web site, mobile application, and ATM — which has significantly accrued the instances and modes of fraud. this is where artificial intelligence (AI) and Machine Learning (ML) come into the picture. These next-gen technologies are ready to assist humans in detecting patterns and taking judgment calls. the need for banking and financial service industry (BFSI) is to tap into the entire potential of AI together with Machine Learning (ML), natural language process (NPL) and Pattern Recognition to not only observe frauds but also catch the fraudsters before any sort of breach had already taken place.  SIGNATURE VERIFICATION GETS SMARTER Signature verification gets smarter by bringing artificial intelligence (AI) into the automation method. Signature verification automation software will play a crucial role here in fraud prevention ensuring that each one documents (checks and other signed documents) undergo signature verification. this could be extremely high volumes of documents many 100s of thousands daily if not more  Applications of neural networks have expanded significantly in recent years from image segmentation to natural language processing to time-series forecasting. One notably successful use of deep learning is embedding, a method used to represent discrete variables as continuous vectors. This technique has found practical applications with word embeddings forAn embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbersNeural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space.  Neural network embeddings have 3 primary purposes:    Finding nearest neighbors in the embedding space. These can be used to make recommendations based on user interests or cluster categories.    As input to a machine learning model for a supervised task.    For visualization of concepts and relations between categories.  This means in terms of the book project, using neural network embeddings, we can take all 37,000 book articles on Wikipedia and represent each one using only 50 numbers in a vector. Moreover, because embeddings are learned, books that are more similar in the context of our learning problem are closer to one another in the embedding space.  Neural network embeddings overcome the two limitations of a common method for representing categorical variables: one-hot encoding.  Limitations of One Hot Encoding  The operation of one-hot encoding categorical variables is actually a simple embedding where each category is mapped to a different vector. This process takes discrete entities and maps each observation to a vector of 0s and a single 1 signaling the specific category.  The one-hot encoding technique has two main drawbacks: For high-cardinality variables — those with many unique categories — the dimensionality of the transformed vector becomes unmanageable. The mapping is completely uninformed: “similar” categories are not placed closer to each other in embedding space. The first problem is well-understood: for each additional category — referred to as an entity — we have to add another number to the one-hot encoded vector. If we have 37,000 books on Wikipedia, then representing these requires a 37,000-dimensional vector for each book, which makes training any machine learning model on this representation infeasible.  The second problem is equally limiting: one-hot encoding does not place similar entities closer to one another in vector space. If we measure similarity between vectors using the cosine distance, then after one-hot encoding, the similarity is 0 for every comparison between entities.  This means that entities such as War and Peace and Anna Karenina (both classic books by Leo Tolstoy) are no closer to one another than War and Peace is to The Hitchhiker’s Guide to the Galaxy if we use one-hot encoding Considering these two problems, the ideal solution for representing categorical variables would require fewer numbers than the number of unique categories and would place similar categories closer to one another. To construct a better representation of categorical entities, we can use an embedding neural network and a supervised task to learn embeddings.Learning Embeddings  The main issue with one-hot encoding is that the transformation does not rely on any supervision. We can greatly improve embeddings by learning them using a neural network on a supervised task. The — weights — of the network which are adjusted to minimize loss on the task. The resulting embedded vectors are representations of categories where similar categories — relative to the task — are closer to one another.  For example, if we have a vocabulary of 50,000 words used in a collection of movie reviews, we could learn 100-dimensional embeddings for each word using an embedding neural network trained to predict the sentimentality of the reviews. (For exactly this application see Words in the vocabulary that are associated with positive reviews such as “brilliant” or “excellent” will come out closer in the embedding space because the network has learned these are both associated with positive reviews.  Introduction to Combine framework in Swift  Understand the basics of the Combine framework with code examples  Combine is Apple’s framework for reactive programming introduced at WWDC19. It allows us to process values over time using declarative APIs that can work as an alternative to the delegate pattern or in-blocks callbacks.  Although, we can use Combine together with other patterns or architectures to solve specific tasks in our projects. In this article, I want to show you an introduction to the framework with some code examples so you can see the potential that Combine has.    Photo by Maxwell Nelson on Unsplash  There’re three main concepts in the Combine framework that we need to understand before we dive deep into it.  Publishers  It’s a protocol that defines the values type that are going to be emitted over time. The events that a Publisher can emit are a value, an error, and a successful completion.  The protocol has two associated types that must be defined:  Output: The value types that will be sent. It could be a String, an Int, a custom Struct, etc.  Failure: The error type to use when we want to emit a failure event.  Subscribers  A Subscriber can receive the events that a Publisher emits and perform the task needed with those events.  It’s also a protocol and, as the Publisher, has two associated types :  Input: The value type that the Subscriber is going to receive.  Failure: The error type when the Publisher emits a failure event.  Both the Input and the Failure must match the Publisher’s Output and Failure types.  Operators  There are methods that can transform the values that a Publisher emits. We can chain as many operators as we want. For example, if we’re making a network request and we get a 500 error, we could capture that error, transform it to a custom Error type, and send that value.  You can check the full list of the operators that Combine has here.  Combine in Action  Now that we have covered the basics, let’s jump into some code examples to see how Combine actually works.  We will be using AnyPublisher type, which is an implementation of the Publisher protocol that we can use to return the Publisher instance that we want. Also, the sink method allows us to process the values that the Publisher sends.  Convenience Publishers  We can take advantage of some of the built-in Publishers that Combine provides instead of creating our own. Let’s see the most common ones.  Just  Emits a value only one time, then finishes. This type of Publisher is useful when we want to replace an error with some default value.    Fail  The Publisher ends with a specific error. For example, if in our previous example we wouldn’t want to return a default value when getting an error but a specific error instead, we can use the Fail Publisher.    Future  It might or might not emit a value and finish or fail. It’s useful for asynchronous code, like API calls. We need to fulfill a promise with some result, it could be successful or failure. If the promise is fulfilled, that promise is sent to the subscriber.    Subjects  It’s a subclass of the Publisher protocol that can emit values on demand to the Subscribers by calling the send(_:)method. The framework already has two Subjects implemented that are ready to go.  Imagine that we’re working on an application to see the live score and announcements of a soccer match.    A pretty simple struct that is built with a specific Subject. Once we create an instance, we can emit a new announcement by calling the sendAnnouncement method. Also, we can tell the subscribers that the match ended or we can emit a custom error if something was wrong using matchEnded and matchSuspended methods.  PassThroughSubject  What the PassThroughSubject basically does, is emits new values to the subscribers without storing any state or values itself.    CurrentValueSubject  It’s very similar to PassThroughSubject with a key difference, CurrentValueSubject does store the actual value and it publishes it to the subscribers whenever that value changes. Because it stores the current value, any new subscriber will receive that value at the moment of the subscription.  Another difference is that CurrentValueSubject has an initial value with which we need to initialize the Subject.    Published Wrapper  By using the @Published wrapper in any property, Combine will automatically create a Publisher for that property and it will emit the value whenever the property value change. We can access the Publisher with the $ operator.  The only restriction that we have is that @Published can be used in classes, not in structs.    Summary  Combine is a powerful framework if we want to use reactive programming in our projects, but it comes with an expensive learning curve at the beginning. However, once you fully understand the basics it becomes easier to use.  Having said that, not every project is suitable for Combine, you need to be sure if you want to adopt this reactive approach or if any other architecture or framework fits better your project’s need.  DispatchGroup and DispatchSemaphore in Swift    Callback hell is like a Hadouken in your code  During our development time, we must be facing some cases that need our task executed in a specific order, in the most common requirement is to execute the task after another one.  And the most common way is to do the nested callback with Closure(in objective-C, we call it “Block”). with more closure in the nested callback, your code will more move to the right, leading to what we call “Callback hell” like below example, this kind of code is quite hard to read and maintain.    Callback hell example.  The above image is a very common example, such the case we need to reload UI after all APIs responded, is very easy to make it as a Callback hell, Currently we have lots of solutions for callback hell in these days, such as DispatchGroup and DispatchSemaphore, which we will discuss later.  Other solutions like third party framework - Promise and async/await which Apple has introduced in 2021 WWDC.   These solutions can improve the callback hell issue efficiently.  DispatchGroup  DispatchGroup is an object that can monitor all the concurrent tasks you want to track.  Through execute group.enter when the task has started and execute group.leave when the task is complete, DispatchGroup can tell you which task has been completed, so you can execute the final task via calling group.notify, for execute the final task after all the tasks you monitor have been done.  For example, if we need to reload UI after all of the API has responses, we can achieve it via using DispatchGroup.    Example of DispatchGroup  The result as below, as you can see, reload user interface has been executed after all of the APIs are finished.  fetchUserInfo(completion:) start  fetchWeatherInfo(completion:) start  loginAPI(completion:) start  fetchUserInfo(completion:) completed  fetchWeatherInfo(completion:) completed  loginAPI(completion:) completed  Reload user interface.  DispatchSemaphore  There is another useful thing called DispatchSemaphore which can design how many concurrent threads can be executed at the same time. DispatchSemaphore is like a resource control canter, by setting value of DispatchSemaphore.init(value: Int), we can tell DispatchSemaphore how many concurrent threads we want to strict to execute at the same time, by using DispatchSemaphore.wait(), will occupy resource and reduce one resource count(value -1), and when the task has done, by calling DispatchSemaphore.signal(), will release resource and recover one resource count(value +1).  Please note, the thread will be blocked if the DispatchSemaphore value is negative until .signal call enough for makes value positive.  There is two way we can use DispatchSemaphore according to Apple documentation.  Passing 0 to DispatchSemaphore value for when two threads have to reconcile the completion of a specific event.  Passing greater than 0 to DispatchSemaphore value to control the resource.  For the first way, we use the DispatchSemaphore by passing 0 to the value.    Start fetching user information.  Fetch user information completed.  Start reload UI.  In this example, we want to wait for fetchUserInfo completed, so we can do the reloadUI.   Of course, you can put reloadUI into fetchUserInfo to meet the requested, but in some cases, you know some code we should not put into closure for accountability reasons.  In such kind of cases, DispatchSemaphore provides another way to let us have a more flexible choice to write the code.   In line 11, we create a DispatchSemaphore and pass 0 to its value, execute fetchUserInfo in line 13 and call .wait() in line 17 to reduce 1 to value(makes value equal to -1), once the value is negative, the thread is blocked.  When fetchUserInfo is completed and call .signal(), the value will increase 1(makes value equal to 0), the thread will be unblocked, and continue to execute reloadUI in line 18.  The second way to use DispatchSemaphore, is bypassing a number greater than 0 to DispatchSemaphore value.  For example, if a user wants to download 15 files and we want to schedule download these files three by three instead download them all at the same time, we can implement with DispatchSemaphore to meet the requirement.    Result below:  Downloading 1 task  Downloading 2 task  Downloading 3 task  Download 1 task completed  Downloading 4 task  Download 4 task completed  Downloading 5 task  Download 3 task completed  Downloading 6 task  Download 2 task completed  Downloading 7 task  Download 5 task completed  Download 6 task completed  Downloading 8 task  Downloading 9 task  Download 8 task completed  Downloading 10 task  Download 9 task completed  Downloading 11 task  Download 7 task completed  Downloading 12 task  Download 11 task completed  Downloading 13 task  Download 10 task completed  Downloading 14 task  Download 14 task completed  Download 13 task completed  Downloading 15 task  Download 12 task completed  Download 15 task completed  As you can see above result, we can strict our resources, to control how many threads we can execute at the same time very easily, once a thread has been completed, the other task will continue to execute immediately, to remain three tasks activated at the same time until all the download task has finished.  How To Get A Job In iOS    Worlds First iOS Developer | CEO of iOSBFree  How To Get A Job In iOS  With iOS Developers being in such high demand it has never been easier to get a job as it is right now. Apple is investing in new devices and a considerable amount of R&D (Research & Development), which means that this growth will only increase over the next few years.  If you are an aspiring new iOS developer and you’re looking for your next role then we may have some tips that might interest you!   LinkedIn  Linked In is a great tool for both job hunting and building a professional business network. You won't even have to perform the searching yourself as recruiters (both internal and external) will contact you about potential opportunities and iOS roles. We 100% recommend building up your LinkedIn network and profile page. It’s worth every minute you spend writing a top-notch profile containing every role and highlighting each exciting project where you have been an active contributor etc.  You can also use Linkedin to search for jobs as many companies now advertise there. They’re not posted daily but registering for job notifications is an absolute must if you’re actively searching for new roles.  Recruiters  Recruiters are your best friends. Don’t avoid them. Instead, run towards them! Recruiters directly benefit from your success and receive a hefty commission based on your annual salary. This means that they want you to receive a job offer (with more money) just as much as you do. i.e. it’s in their best interest to help you out.  I’ve had a great deal of experience dealing with recruiters and while I understand the general public opinion can be a little mixed because their approach is extremely forward and very pushy, we must use every available tool in our toolbox. Recruiters are a very powerful tool and if you want to BOOST your career forwards you should add them to your tactical arsenal.   If you want to fast track the interviewing process and get a higher paying role faster then use recruiters!  Write A Better CV  Ensure that you build a 2 page CV with bullet points that list your skills at the beginning of the first page. Reduce the use of written paragraphs and make your CV stand out amongst the crowd with your unique experience.  You must create an award-winning CV because that is how you sell yourself and offer your service to the world. You can, and should think of yourself as a product that needs to be sold to its audience. Perhaps we would all be better off by thinking of this as a marketing problem. Our task then becomes creating an advertising campaign exclusively to sell ourselves and our work.  Our CV is our opportunity to do exactly that!  iOSBFree have created a FREE DOWNLOADABLE CV Guide and editable CV Template just for our students and customers. Why not take a look now?  Get More Career Things  As we discussed in another iOSBFree article you should collect as many Career Things as possible and list them all (bullet points) on your CV. Make them bold. Make them stand out. In fact, smack the reader in the face with your amazing contribution to previous roles so your application burns itself into their mind. By shouting about our Career Thing or multiple Career Things, we are labelling ourselves with some sort of fact or activity. i.e. This CV was the person who did X at Y!  For me personally, I am one of the first iOS developers in the world and that is how I am guaranteed an interview every single time my CV is passed across someone's desk.  A Career Thing is something that we have accomplished in our careers.   e.g. “I increased profits by £500,000 by focussing on the conversion of user registration when users first open the app”  For any readers out there who do not yet have any Career Things do not fear or become demotivated, simply set a goal (successful people set goals) to get some! Decide on what you want to achieve. Set a 6-month timeline to achieve it and then go achieve it! Good luck!  How To Start A Career In iOS    Worlds First iOS Developer | CEO of iOSBFree  How To Start A Career In iOS  Let’s start this article by answering this question directly and concisely  Q: So what do I do first as a beginner?  A: Study and learn!  Studying  Keen and motivated engineers simply want to get their hands dirty by jumping straight into the code. But we must first be patient! It’s almost impossible within any skilled area to become good or even to an acceptable level if we have little or no experience in it. If we’re to build a solid career in iOS then we must understand both the basics and the fundamentals of software engineering.  Online Courses  Online courses are great! They are cheap, affordable, mobile-friendly and can be studied by anyone in any country or location in the world. With companies such as Udemy and LinkedIn Learning shifting the marketplace away from outdated academic institutions such as Universities it has never been easier to learn new skills directly from professionals (with decades of industry experience) in a small amount of time.  Why not try our FREE ONLINE COURSES?  Start Small  We recommend offering your services as soon as you can. It can be a startup, people using online platforms such as UpWork or Fiver and anyone that requires cheap Labour. We recommend forgetting the money initially. That comes later. First, you’ll need the skills and learning that only come from hands-on experience. To gain the skills that you need you must spend time on the computer writing as many apps as you can.  Experience is everything!  Xcode Playgrounds  We use Xcode playground files to test certain behaviour with many of our online courses. They are simply wonderful as a learning tool because they provide focus to the exact area you wish to learn. There are no distractions other than the few lines of code you are trying to understand.  It’s exceptionally fast to try out and test behaviour of newly discovered syntax and core engineering concepts that most developers never learn. For example, have you learned about ARC and how the underlying memory works? Why not try to prove how you would prevent a retain cycle by writing a quick method in a playground file?  We 100% recommend writing code within an Xcode playground file and practising core language features each and every week. Or perhaps even once a day. Remember, the more you study the faster you will learn!  Study The Fundamentals  Don’t just focus on what’s fun and exciting such as animations and cool graphics. The Core Graphics framework can be extremely fun and interesting but in an interview situation you’ll be discussing ARC memory management, reference & value types, variable capture within closures and architectural design patterns. Do you understand each of these concepts yet?  The fundamentals can be taught fairly quickly and without much effort really. By using a simple Xcode playground file it could take less than one hour to practice some core fundamentals that some developers will never truly understand. You’d be surprised at how few developers take the time to actually do this!  Swift | Reaproveitando Views utilizando State Design Pattern  Hoje quero compartilhar com vocês um cenário que é extremamente comum em diversas aplicações, que á primeira vista parece simples, mas que permite aplicar diversos conceitos importantes de programação, em especial relacionados a design patterns e princípios SOLID.  Vamos começar analisando o cenário em questão através da imagem abaixo:    Basicamente temos aqui um cadastro em multi-step, onde capturamos uma informação do usuário por vez.  Logo de início conseguimos notar que embora as informações captadas, bem como os textos, sejam diferentes, o layout de todas as telas é praticamente o mesmo.  Partindo disso, vamos listar os pontos de diferença entre cada uma das telas:  Informações textuais tais como: título, placeholder do textField e resumo das informações do usuário;  Máscara de textField bem como possíveis regras de validação da entrada do usuário;  Ação associada ao botão de continuar.  Dado o contexto, vamos levantar o problema que desejamos solucionar:  Como separar os diferentes comportamentos de cada tela de maneira efetiva e reaproveitando a camada de View (UIView e UIViewController)?  Existem diversas boas maneiras de resolver esse problema, aqui nesse artigo vamos optar por utilizar o design pattern chamado State. A ideia será transformar cada tipo de informação que desejamos recuperar em um estado para nossa camada de View. Nesse artigo essa camada será representada por uma classe chamada FormViewController.  Vamos começar criando um protocolo que irá representar as capacidades que cada estado precisa ter, que no caso são as que listamos mais acima nesse artigo:    Em seguida vamos criar as classes concretas, que irão implementar o protocolo FormState que acabamos de criar e que representarão cada um dos estados de input de informações:    Aqui vale ressaltar dois pontos extremamente importantes:  1. Adição de método init que recebe uma UIViewController  Esse ponto é importante pois aqui estamos utilizando injeção de dependência para fornecer para nossas classes de estado uma referência de FormViewController que contém as dependências necessárias para realizarmos tanto as modificações textuais como as validações de textField e ações do botão continuar.  2. Não inclusão do método init ao protocolo FormState  Aqui a ideia é evitar qualquer tipo de acoplamento ao nosso protocolo State, ele deve conter apenas abstrações dos comportamentos esperados para cada estado.  Dando sequência ao código, por fim, nós iremos adicionar à nossa classe FormViewController, que é a classe que contém toda parte de layout reaproveitável, uma variável do tipo FormState e chamaremos os métodos desse protocolo nos locais apropriados:    Fazendo isso a classe FormViewController, que contém todo layout reaproveitável, passa a facilmente conseguir se adequar a diferentes estados por intermédio de um objeto FormState, além de permitir que novas telas possam ser adicionadas a esse fluxo de formulário, sendo necessário apenas criar novas classes que implementem o protocolo FormState, baseado no Open/Closed principle do SOLID.  Protocolos são ferramentas extremamente poderosas que nos permitem criar diferentes níveis de abstração para aplicar e adequar os múltiplos design patterns que vemos na literatura, além de nos ajudar a compreender e aplicar os conceitos contidos dentro SOLID.  Por hoje é isso pessoal! :)  